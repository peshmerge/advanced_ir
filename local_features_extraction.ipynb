{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting local features for one file\n",
    "The code in the cell below is made to extract local features for only one image. This code is used to test things first before going large scale to the whole database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import os\n",
    "import sys\n",
    "import timm\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from torchinfo import summary\n",
    "import numpy as np\n",
    "import torchvision.transforms as ttf\n",
    "import numpy as np\n",
    "import csv\n",
    "# !pip install nbformat\n",
    "%run model_notebook.ipynb\n",
    "\n",
    "# Setup the paths and constants\n",
    "BACKBONE = \"resnext\"\n",
    "POOL = \"GeM\"\n",
    "NORM = None\n",
    "BATCH_SIZE = 1\n",
    "\n",
    "# Choose the image you want to use (the corresponding query file will be loaded)\n",
    "# image_file_name = 'x3vA7Bk0HNI6rGkDpDZQUQ'\n",
    "image_file_name = 'SQNDJeXa8UQ9pHht-13PNg'\n",
    "\n",
    "root_dir = 'global_feature_verification_dir/'\n",
    "q_idx = os.path.join(root_dir,'cph',image_file_name+'_query.json')\n",
    "\n",
    "# Convert any given npy features file to csv file\n",
    "def convert_npy_features_to_csv(source_file, target_file):\n",
    "    source_features = np.load(source_file)\n",
    "    source_features_list = source_features.tolist()\n",
    "\n",
    "    with open(target_file, \"w\") as f:\n",
    "        wr = csv.writer(f)\n",
    "        wr.writerows(source_features_list)\n",
    "\n",
    "def extract_features(model, local_branch, feats_file , f_length=2048):\n",
    "    # To run the feature extraction just like nicola we need the following things:\n",
    "    image_size = [480,640]\n",
    "    image_t = ttf.Compose([ttf.Resize(size=(image_size[0],image_size[1])),\n",
    "                        ttf.ToTensor(),\n",
    "                        ttf.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "                        ])\n",
    "    dl = create_dataloader(\"test\",root_dir, q_idx,None,image_t, BATCH_SIZE)\n",
    "    \n",
    "    if not os.path.exists(feats_file):\n",
    "        feats = np.zeros((len(dl.dataset), f_length))\n",
    "        for i, batch in tqdm(enumerate(dl), desc=\"Extracting features\"):\n",
    "            local_features = model.forward(batch.cuda())\n",
    "            local_features = local_branch(local_features)\n",
    "            feats[i * dl.batch_size:i * dl.batch_size + dl.batch_size] = local_features.cpu().detach().squeeze(0)\n",
    "        np.save(feats_file, feats)\n",
    "        print(f\"{feats_file} has been saved..........\")\n",
    "    else:\n",
    "        print(feats_file,\"already exists. Skipping.\")\n",
    "\n",
    "\n",
    "## Extract Local features using using the model from the disk \n",
    "model_file_weights = os.path.join('generalized_contrastive_loss','Models','MSLS','MSLS_resnext_GeM_480_GCL.pth')\n",
    "model = create_model(BACKBONE,POOL,norm=None,mode=\"single\" )\n",
    "file_name_extension = ''\n",
    "try:\n",
    "    model.load_state_dict(torch.load(model_file_weights)[\"model_state_dict\"])\n",
    "except:\n",
    "    model.load_state_dict(torch.load(model_file_weights)[\"state_dict\"])\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()\n",
    "\n",
    "# Force inference mode\n",
    "model.eval()\n",
    "\n",
    "local_branch = LocalBranch(input_dim=1024, out_channel=2048)\n",
    "local_branch.cuda()\n",
    "\n",
    "feats_file= os.path.join(root_dir,'cph',image_file_name+'_local_feature_file'+ file_name_extension+'.npy')\n",
    "csv_file= os.path.join(root_dir,'cph',image_file_name+'_local_feature_file'+file_name_extension +'.csv')\n",
    "\n",
    "extract_features(model,local_branch, feats_file)\n",
    "\n",
    "# We are here, it means the npy is saved, convert to csv file\n",
    "convert_npy_features_to_csv(feats_file, csv_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracing local features for the whole dataset\n",
    "The code in the cells below is used to extract local features for all query and map images for both CPH (Copenhagen) and SF (San Francisco)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from torchinfo import summary\n",
    "import numpy as np\n",
    "import torchvision.transforms as ttf\n",
    "import numpy as np\n",
    "import csv\n",
    "%run model_notebook.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# This function takes in a ResNextGCL model and loop through all given images and extract local features.\n",
    "# The local features are generated as follows. Lf generated using GCL --> passed to MultiAttrous --> attention map generated -->\n",
    "# a 2048 lf vector is generated and saved for each image. \n",
    "def extract_features(dl, model, feats_file , f_length=2048):\n",
    "    # Define local branch model for each city and each query and map for the given city in the dataset\n",
    "    local_branch = LocalBranch(input_dim=1024, out_channel=2048)\n",
    "    # we must do this otherwise it will perform it on the cpu      \n",
    "    local_branch.cuda()\n",
    "    if not os.path.exists(feats_file):\n",
    "        feats = np.zeros((len(dl.dataset), f_length))\n",
    "        for i, batch in tqdm(enumerate(dl), desc=\"Extracting features\"):\n",
    "            # obtain lf first using ResNext + GCL\n",
    "            local_features = model.forward(batch.cuda())\n",
    "            # Pass those lf to local branch model\n",
    "            local_features = local_branch(local_features)\n",
    "            # squeeze and detach to endup with [2048] vector\n",
    "            feats[i * dl.batch_size:i * dl.batch_size + dl.batch_size] = local_features.cpu().detach().squeeze(0)\n",
    "        np.save(feats_file, feats)\n",
    "        print(f\"{feats_file} has been saved..........\")\n",
    "    else:\n",
    "        print(feats_file,\"already exists. Skipping.\")\n",
    "\n",
    "\n",
    "#extract features for the whole dataset\n",
    "def extract_features_msls(\n",
    "    model,\n",
    "    subset='val',\n",
    "    root_dir = 'generalized_contrastive_loss/msls/',\n",
    "    weights_file_path = 'generalized_contrastive_loss/Models/MSLS/MSLS_resnext_GeM_480_GCL.pth',\n",
    "    f_length = 2048,\n",
    "    results_dir = 'generalized_contrastive_loss/results/MSLS/val/',\n",
    "    batch_size = 1,\n",
    "    k = 30,\n",
    "    ):\n",
    "    cities = [\"cph\", \"sf\"]\n",
    "\n",
    "    savename= 'MSLS_resnext_GCL_multi_attrous_attention_map'\n",
    "    \n",
    "    subset_dir=subset if subset == \"test\" else \"train_val\"\n",
    "    image_size = [480,640]\n",
    "    image_t = ttf.Compose([ttf.Resize(size=(image_size[0],image_size[1])),\n",
    "                        ttf.ToTensor(),\n",
    "                        ttf.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "                        ])\n",
    "    for c in cities:\n",
    "        print(c)\n",
    "        m_raw_file = root_dir+subset_dir+\"/\"+c+\"/database/raw.csv\"\n",
    "        q_idx_file = root_dir+subset_dir+\"/\"+c+\"/query.json\"\n",
    "        m_idx_file = root_dir+subset_dir+\"/\"+c+\"/database.json\"\n",
    "        q_dl = create_dataloader(\"test\", root_dir, q_idx_file, None, image_t, batch_size)\n",
    "        q_feats_file =results_dir+\"/\"+savename+\"_\"+c+\"_local_queryfeats.npy\"\n",
    "        #extract features for the query images\n",
    "        extract_features(q_dl, model, q_feats_file,  f_length)\n",
    "        \n",
    "        m_dl = create_dataloader(\"test\", root_dir, m_idx_file, None, image_t, batch_size)\n",
    "        m_feats_file =results_dir+\"/\"+savename+\"_\"+c+\"_local_mapfeats.npy\"\n",
    "        #extract features for the map images \n",
    "        extract_features(m_dl, model, m_feats_file, f_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/jovyan/.cache/torch/hub/facebookresearch_WSL-Images_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " the layers of the resnext101_32x8d_wsl are: odict_keys(['conv1', 'bn1', 'relu', 'maxpool', 'layer1', 'layer2', 'layer3', 'layer4', 'avgpool', 'fc'])\n",
      " the layers of the resnext101_32x8d_wsl are after removing the last two layers (avgpool and fc): odict_keys(['0', '1', '2', '3', '4', '5', '6', '7'])\n",
      "Number of layers: 8\n",
      "0 Conv2d IS TRAINED\n",
      "1 BatchNorm2d IS TRAINED\n",
      "2 ReLU IS TRAINED\n",
      "3 MaxPool2d IS TRAINED\n",
      "4 Sequential IS TRAINED\n",
      "5 Sequential IS TRAINED\n",
      "6 Sequential IS TRAINED\n",
      "7 Sequential IS TRAINED\n",
      "cph\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features: 6595it [03:01, 36.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generalized_contrastive_loss/results/MSLS/val//MSLS_resnext_GCL_multi_attrous_attention_map_cph_local_queryfeats.npy has been saved..........\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features: 12601it [05:29, 38.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generalized_contrastive_loss/results/MSLS/val//MSLS_resnext_GCL_multi_attrous_attention_map_cph_local_mapfeats.npy has been saved..........\n",
      "sf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features: 4525it [01:58, 38.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generalized_contrastive_loss/results/MSLS/val//MSLS_resnext_GCL_multi_attrous_attention_map_sf_local_queryfeats.npy has been saved..........\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features: 6315it [02:45, 38.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generalized_contrastive_loss/results/MSLS/val//MSLS_resnext_GCL_multi_attrous_attention_map_sf_local_mapfeats.npy has been saved..........\n",
      "CPU times: user 12min 49s, sys: 34.4 s, total: 13min 23s\n",
      "Wall time: 13min 28s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Convert any given npy features file to csv file. THis is a helper function to inspect the generated npy lf file\n",
    "def convert_npy_features_to_csv(source_file, target_file):\n",
    "    source_features = np.load(source_file)\n",
    "    source_features_list = source_features.tolist()\n",
    "    with open(target_file, \"w\") as f:\n",
    "        wr = csv.writer(f)\n",
    "        wr.writerows(source_features_list)\n",
    "\n",
    "# Setup the paths and constants\n",
    "BACKBONE = \"resnext\"\n",
    "POOL = \"GeM\"\n",
    "NORM = None\n",
    "\n",
    "## Extract Local features using the model from the disk \n",
    "model_file_weights = os.path.join('generalized_contrastive_loss','Models','MSLS','MSLS_resnext_GeM_480_GCL.pth')\n",
    "model = create_model(BACKBONE,POOL,norm=None,mode=\"single\" )\n",
    "file_name_extension = ''\n",
    "try:\n",
    "    model.load_state_dict(torch.load(model_file_weights)[\"model_state_dict\"])\n",
    "except:\n",
    "    model.load_state_dict(torch.load(model_file_weights)[\"state_dict\"])\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()\n",
    "\n",
    "# Force inference mode\n",
    "model.eval()\n",
    "\n",
    "extract_features_msls(model)\n",
    "\n",
    "# We are here, it means the npy is saved, convert to csv file\n",
    "# convert_npy_features_to_csv(feats_file, csv_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "apfir",
   "language": "python",
   "name": "apfir"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "cbf36edf16826f338fcddf9817feee204e82a9d331b7cc35297b5ee4d265a83d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
