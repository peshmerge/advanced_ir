{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting local features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import os\n",
    "import sys\n",
    "import timm\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from torchinfo import summary\n",
    "import numpy as np\n",
    "import torchvision.transforms as ttf\n",
    "import numpy as np\n",
    "import csv\n",
    "# !pip install nbformat\n",
    "%run model_notebook.ipynb\n",
    "\n",
    "# Setup the paths and constants\n",
    "BACKBONE = \"resnext\"\n",
    "POOL = \"GeM\"\n",
    "NORM = None\n",
    "BATCH_SIZE = 1\n",
    "\n",
    "# Choose the image you want to use (the corresponding query file will be loaded)\n",
    "# image_file_name = 'x3vA7Bk0HNI6rGkDpDZQUQ'\n",
    "image_file_name = 'SQNDJeXa8UQ9pHht-13PNg'\n",
    "\n",
    "root_dir = 'global_feature_verification_dir/'\n",
    "q_idx = os.path.join(root_dir,'cph',image_file_name+'_query.json')\n",
    "\n",
    "# Convert any given npy features file to csv file\n",
    "def convert_npy_features_to_csv(source_file, target_file):\n",
    "    source_features = np.load(source_file)\n",
    "    source_features_list = source_features.tolist()\n",
    "\n",
    "    with open(target_file, \"w\") as f:\n",
    "        wr = csv.writer(f)\n",
    "        wr.writerows(source_features_list)\n",
    "\n",
    "def extract_features(net, feats_file,f_length=1024):\n",
    "    # To run the feature extraction just like nicola we need the following things:\n",
    "    image_size = [480,640]\n",
    "    image_t = ttf.Compose([ttf.Resize(size=(image_size[0],image_size[1])),\n",
    "                        ttf.ToTensor(),\n",
    "                        ttf.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "                        ])\n",
    "    dl = create_dataloader(\"test\",root_dir, q_idx,None,image_t, BATCH_SIZE)\n",
    "    \n",
    "    if not os.path.exists(feats_file):\n",
    "        feats = np.zeros((len(dl.dataset), f_length))\n",
    "        for i, batch in tqdm(enumerate(dl), desc=\"Extracting features\"):\n",
    "            local_features,_ = net.forward(batch.cuda())\n",
    "            feats[i * dl.batch_size:i * dl.batch_size + dl.batch_size] = local_features.cpu().detach().squeeze(0)\n",
    "        np.save(feats_file, feats)\n",
    "        print(f\"{feats_file} has been saved..........\")\n",
    "    else:\n",
    "        print(feats_file,\"already exists. Skipping.\")\n",
    "\n",
    "\n",
    "## Extract Local features using using the model from the disk \n",
    "model_file_weights = os.path.join('generalized_contrastive_loss','Models','MSLS','MSLS_resnext_GeM_480_GCL.pth')\n",
    "model = create_model(BACKBONE,POOL,norm=None,mode=\"single\" )\n",
    "file_name_extension = ''\n",
    "try:\n",
    "    model.load_state_dict(torch.load(model_file_weights)[\"model_state_dict\"])\n",
    "except:\n",
    "    model.load_state_dict(torch.load(model_file_weights)[\"state_dict\"])\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()\n",
    "\n",
    "# Force inference mode\n",
    "model.eval()\n",
    "\n",
    "feats_file= os.path.join(root_dir,'cph',image_file_name+'_local_feature_file'+ file_name_extension+'.npy')\n",
    "csv_file= os.path.join(root_dir,'cph',image_file_name+'_local_feature_file'+file_name_extension +'.csv')\n",
    "\n",
    "extract_features(model,feats_file)\n",
    "\n",
    "# We are here, it means the npy is saved, convert to csv file\n",
    "convert_npy_features_to_csv(feats_file, csv_file)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracing local features for the whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import os\n",
    "import sys\n",
    "import timm\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from torchinfo import summary\n",
    "import numpy as np\n",
    "import torchvision.transforms as ttf\n",
    "import numpy as np\n",
    "import csv\n",
    "# !pip install nbformat\n",
    "%run model_notebook.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  extract_features_msls(params.subset, params.root_dir, test_net, f_length, image_t, savename, results_dir, params.batch_size, 30)\n",
    "\n",
    "# python3 extract_predictions.py --dataset MSLS --root_dir \"/home/jovyan/AIR/generalized_contrastive_loss/MSLS/\"\n",
    "#  --subset val --model_file Models/MSLS/MSLS_resnext_GeM_480_GCL.pth --backbone resnext --pool GeM --f_length 2048 --batch_size 5\n",
    "\n",
    "def extract_features(dl, model,f_length,  feats_file,):\n",
    "    if not os.path.exists(feats_file):\n",
    "        feats = np.zeros((len(dl.dataset), f_length))\n",
    "        for i, batch in tqdm(enumerate(dl), desc=\"Extracting features\"):\n",
    "            local_features,_ = model.forward(batch.cuda())\n",
    "            feats[i * dl.batch_size:i * dl.batch_size + dl.batch_size] = local_features.cpu().detach().squeeze(0)\n",
    "        np.save(feats_file, feats)\n",
    "        print(f\"{feats_file} has been saved..........\")\n",
    "    else:\n",
    "        print(feats_file,\"already exists. Skipping.\")\n",
    "\n",
    "\n",
    "def extract_features_msls(\n",
    "    model,\n",
    "    subset='val',\n",
    "    root_dir = 'generalized_contrastive_loss/MSLS/',\n",
    "    weights_file_path = 'Models/MSLS/MSLS_resnext_GeM_480_GCL.pth',\n",
    "    f_length = 1024,\n",
    "    results_dir = 'generalized_contrastive_loss/results/MSLS/val/',\n",
    "    batch_size = 1,\n",
    "    k = 30,\n",
    "    ):\n",
    "    cities = [\"cph\", \"sf\"]\n",
    "\n",
    "    savename= weights_file_path.split(\"/\")[-1].split(\".\")[0]\n",
    "    \n",
    "    subset_dir=subset if subset == \"test\" else \"train_val\"\n",
    "    image_size = [480,640]\n",
    "    image_t = ttf.Compose([ttf.Resize(size=(image_size[0],image_size[1])),\n",
    "                        ttf.ToTensor(),\n",
    "                        ttf.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "                        ])\n",
    "    for c in cities:\n",
    "        print(c)\n",
    "        m_raw_file = root_dir+subset_dir+\"/\"+c+\"/database/raw.csv\"\n",
    "        q_idx_file = root_dir+subset_dir+\"/\"+c+\"/query.json\"\n",
    "        m_idx_file = root_dir+subset_dir+\"/\"+c+\"/database.json\"\n",
    "        q_dl = create_dataloader(\"test\", root_dir, q_idx_file, None, image_t, batch_size)\n",
    "        q_feats_file =results_dir+\"/\"+savename+\"_\"+c+\"_local_queryfeats.npy\"\n",
    "        \n",
    "        extract_features(q_dl, model, f_length, q_feats_file)\n",
    "        \n",
    "        m_dl = create_dataloader(\"test\", root_dir, m_idx_file, None, image_t, batch_size)\n",
    "        m_feats_file =results_dir+\"/\"+savename+\"_\"+c+\"_local_mapfeats.npy\"\n",
    "        \n",
    "        extract_features(m_dl, model, f_length, m_feats_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Convert any given npy features file to csv file\n",
    "def convert_npy_features_to_csv(source_file, target_file):\n",
    "    source_features = np.load(source_file)\n",
    "    source_features_list = source_features.tolist()\n",
    "\n",
    "    with open(target_file, \"w\") as f:\n",
    "        wr = csv.writer(f)\n",
    "        wr.writerows(source_features_list)\n",
    "\n",
    "# Setup the paths and constants\n",
    "BACKBONE = \"resnext\"\n",
    "POOL = \"GeM\"\n",
    "NORM = None\n",
    "\n",
    "## Extract Local features using using the model from the disk \n",
    "model_file_weights = os.path.join('generalized_contrastive_loss','Models','MSLS','MSLS_resnext_GeM_480_GCL.pth')\n",
    "model = create_model(BACKBONE,POOL,norm=None,mode=\"single\" )\n",
    "file_name_extension = ''\n",
    "try:\n",
    "    model.load_state_dict(torch.load(model_file_weights)[\"model_state_dict\"])\n",
    "except:\n",
    "    model.load_state_dict(torch.load(model_file_weights)[\"state_dict\"])\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()\n",
    "\n",
    "# Force inference mode\n",
    "model.eval()\n",
    "\n",
    "extract_features_msls(model)\n",
    "\n",
    "# We are here, it means the npy is saved, convert to csv file\n",
    "# convert_npy_features_to_csv(feats_file, csv_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3813",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cbf36edf16826f338fcddf9817feee204e82a9d331b7cc35297b5ee4d265a83d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
