{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a868493c-43f4-4d6e-90aa-9c60edfb0194",
   "metadata": {},
   "source": [
    "# Re-Ranking Using Global and Local Features\n",
    "### UNIVERSITY OF TWENTE\n",
    "\n",
    "Notebook expanding on the concept of re-ranking through the use of local features and global features along with the fusion of such features. Researchers of this notebook include:\n",
    "- NIschal Neupane (n.neupane@student.utwente.nl)\n",
    "- Peshmerge Morad (p.morad@student.utwente.nl)\n",
    "- Aditya Poozhiyil (adityaretissinpoozhiyil@student.utwente.nl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdda6bbd-7d57-4e09-862f-cafcf6c1fbbc",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Imports and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65537146-eb09-48a0-b4ef-ca8d1f0836b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "import cv2\n",
    "import torch\n",
    "import random\n",
    "import runpy\n",
    "import subprocess\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "\n",
    "import torchvision.transforms as ttf\n",
    "\n",
    "from torchinfo import summary\n",
    "\n",
    "# Setup system paths relative to the current file\n",
    "sys.path.insert(1, os.path.join(os.getcwd(), 'generalized_contrastive_loss', 'labeling'))\n",
    "sys.path.insert(2, os.path.join(os.getcwd(), 'generalized_contrastive_loss', 'mapillary_sls'))\n",
    "\n",
    "# Import the other notebook file with models\n",
    "%run model_notebook.ipynb\n",
    "\n",
    "random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8206f4e2-3fac-4583-a499-17df8c98af9c",
   "metadata": {},
   "source": [
    "#### Setup GPU for Cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ade00629-3e43-44c4-a6c3-23c17c2f9284",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting the device to cuda:1...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cpu')\n",
    "display_devices = False\n",
    "\n",
    "# If cuda is available...\n",
    "if torch.cuda.is_available():\n",
    "    # Find GPU with most free memory and set that as the device\n",
    "    mem_usage_list = [torch.cuda.mem_get_info(f'cuda:{gpu_num}')[0] for gpu_num in range(torch.cuda.device_count())]\n",
    "    most_free = mem_usage_list.index(max(mem_usage_list))\n",
    "    device = torch.device(f'cuda:{most_free}')\n",
    "    print(f'Setting the device to {device}...\\n')\n",
    "\n",
    "    if display_devices:\n",
    "        # Print GPU info on all\n",
    "        for gpu_num in range(torch.cuda.device_count()):\n",
    "            available_mem, total_mem = torch.cuda.mem_get_info(f'cuda:{gpu_num}')\n",
    "            print(f'cuda:{gpu_num}')\n",
    "            print('Memory Usage:')\n",
    "            print('Total:', round(total_mem/1024**3,2), 'GB')\n",
    "            print('Allocated:', round((total_mem-available_mem)/1024**3,2), 'GB')\n",
    "            print('Free:   ', round(available_mem/1024**3,2), 'GB')\n",
    "            print()\n",
    "    # Set the default tensor type to gpu\n",
    "    torch.set_default_tensor_type(torch.cuda.FloatTensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be6242e-6648-4c13-a31a-e1d348c5bad7",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Set the appropriate environment variables and constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1784ee1-735c-433c-8be5-c19770365570",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['MAPILLARY_ROOT'] = \"/home/jovyan/APFIR/generalized_contrastive_loss/mapillary_sls/\" # set the mapillary root \n",
    "os.environ['LD_LIBRARY_PATH'] = \"/opt/miniconda3/lib\" # Set the library path to miniconda3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "0715ae43-d4a1-4dfd-8485-fccc1b9c9708",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indexing Parameters\n",
    "RE_INDEX = False\n",
    "\n",
    "# Model Parameters\n",
    "BACKBONE = \"resnext\"  # backbone for the model\n",
    "POOL = \"GeM\"  # pooling type\n",
    "NORM = \"L2\"  # norm type\n",
    "BATCH_SIZE = 1  # batch size\n",
    "\n",
    "DILATION = True  # Whether to add dilation layer\n",
    "FUSION = False  # Whether to use fusion or not\n",
    "\n",
    "IMG_SIZE = (480, 640)  # Size of input image\n",
    "PARAM_LENGTH = 2048  # Number of features to generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "c1d6879f-459a-48c6-bbcc-6981de3f2a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PATHS\n",
    "MODEL_IMGS = os.path.join(os.getcwd(), \"model_imgs\")\n",
    "RESULT_IMGS = os.path.join(os.getcwd(), \"result_imgs\")\n",
    "FONT_PATH = os.path.join(cv2.__path__[0], \"qt\", \"fonts\", \"DejaVuSans.ttf\")\n",
    "\n",
    "# Check if the model imgs directory exists, if not, create it\n",
    "if not os.path.exists(MODEL_IMGS):\n",
    "    os.makedirs(MODEL_IMGS)\n",
    "# Check if the result imgs directory exists, if not, create it\n",
    "if not os.path.exists(RESULT_IMGS):\n",
    "    os.makedirs(RESULT_IMGS)\n",
    "\n",
    "DATA_ROOT = os.path.join(os.getcwd(), \"generalized_contrastive_loss\")\n",
    "\n",
    "MSLS_MODELS = os.path.join(DATA_ROOT, \"Models\", \"MSLS\")\n",
    "RESULTS_ROOT = os.path.join(DATA_ROOT, \"results\", \"MSLS\", \"val\")\n",
    "PREDICTION_PATH = os.path.join(RESULTS_ROOT, \"MSLS_resnext_GeM_480_GCL_predictions.txt\")\n",
    "\n",
    "MODEL_WEIGHT_NAME = \"MSLS_resnext_GeM_480_GCL\"\n",
    "MODEL_WEIGHTS = os.path.join(MSLS_MODELS, f\"{MODEL_WEIGHT_NAME}.pth\")\n",
    "\n",
    "DATASET_ROOT = os.path.join(DATA_ROOT, \"msls\")\n",
    "DATASET_TEST = os.path.join(DATASET_ROOT, \"test\")\n",
    "DATASET_VAL = os.path.join(DATASET_ROOT, \"train_val\")\n",
    "\n",
    "DATASET_VAL_SF = os.path.join(DATASET_VAL, \"sf\")\n",
    "DATASET_VAL_CPH = os.path.join(DATASET_VAL, \"cph\")\n",
    "\n",
    "model_format = MODEL_WEIGHT_NAME\n",
    "global_feat_format = MODEL_WEIGHT_NAME+\"_{city}\"\n",
    "local_feat_format = MODEL_WEIGHT_NAME+\"_{city}\"\n",
    "results_format = MODEL_WEIGHT_NAME\n",
    "if DILATION:\n",
    "    local_feat_format += \"_dilation\"\n",
    "    results_format += \"_dilation\"\n",
    "    model_format += \"_dilation\"\n",
    "if NORM:\n",
    "    global_feat_format += f\"_{NORM}\"\n",
    "    local_feat_format += f\"_{NORM}\"\n",
    "    results_format += f\"_{NORM}\"\n",
    "    model_format += f\"_{NORM}\"\n",
    "if FUSION:\n",
    "    results_format += \"_fusion\"\n",
    "global_feat_format += \"_global_{db}feats.npy\"\n",
    "local_feat_format += \"_local_{db}feats.npy\"\n",
    "results_format += \"_results.txt\"\n",
    "model_format += \"_{ftype}.png\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e6c6e6-3372-4bcc-9dea-4bb7fdbe1573",
   "metadata": {},
   "source": [
    "----\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb4bce56-5705-4160-bf27-29356c3efa91",
   "metadata": {},
   "source": [
    "## Helper Functions\n",
    "Small function defined to help with code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "4b8d78e3-28b2-4642-86d9-77f061d293c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that allows you to convert a given list of string into an image\n",
    "def text2png(lines, fullpath, color=\"#000\", bgcolor=\"#FFF\", fontfullpath=None,\n",
    "             fontsize=13, leftpadding=3, rightpadding=3, width=200):\n",
    "    font = ImageFont.truetype(fontfullpath, fontsize) if fontfullpath else ImageFont.truetype(\"arial.ttf\", fontsize)\n",
    "\n",
    "    line_height = fontsize\n",
    "    img_height = line_height * (len(lines) + (len(lines)//2))\n",
    "\n",
    "    img = Image.new(\"RGBA\", (width, img_height), bgcolor)\n",
    "    draw = ImageDraw.Draw(img)\n",
    "\n",
    "    x = (width * 0.55)\n",
    "    y = 0\n",
    "    max_cols = 0\n",
    "    for line in lines:\n",
    "        if line:\n",
    "            if RESULT_IMGS in fullpath:\n",
    "                split_text = line.split(\":\")\n",
    "                draw.text((leftpadding, y), split_text[0], color, font=font)\n",
    "                draw.text((leftpadding+x, y), \":\", color, font=font)\n",
    "                draw.text((leftpadding+x+leftpadding, y), split_text[1], color, font=font)\n",
    "            elif MODEL_IMGS in fullpath:\n",
    "                split_text = re.split(r'\\s{6,}', line)\n",
    "                num_cols = len(split_text)\n",
    "                max_cols = max(max_cols, num_cols)\n",
    "                x = (width // max_cols)\n",
    "                for col_num, col_text in enumerate(split_text):\n",
    "                    draw.text((leftpadding+(col_num*x), y), col_text, color, font=font)\n",
    "            else:\n",
    "                draw.text((leftpadding, y), line, color, font=font)\n",
    "        y += line_height + (line_height//2)\n",
    "\n",
    "    img.save(fullpath, dpi=(600, 600))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f061c2-bcf7-4aa1-a2bc-3a420edaa6ca",
   "metadata": {},
   "source": [
    "----\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7138773-9a62-42e7-8ed4-9127192b0da8",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Indexing Dataset\n",
    "Create the json indicies of the images, same as running the following command:\n",
    "> python3 labeling/create_json_idx.py --dataset msls --root_dir \"/home/jovyan/APFIR/generalized_contrastive_loss/msls/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "961b85fc-7062-46b4-9195-92c057b45152",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Json index present for sf.\n",
      "Json index present for austin.\n",
      "Json index present for boston.\n",
      "Json index present for budapest.\n",
      "Json index present for nairobi.\n",
      "Json index present for cph.\n",
      "Json index present for saopaulo.\n",
      "Json index present for tokyo.\n",
      "Json index present for helsinki.\n",
      "Json index present for toronto.\n",
      "Json index present for zurich.\n",
      "Json index present for manila.\n",
      "Json index present for london.\n",
      "Json index present for phoenix.\n",
      "Json index present for amsterdam.\n",
      "Json index present for moscow.\n",
      "Json index present for ottawa.\n",
      "Json index present for amman.\n",
      "Json index present for berlin.\n",
      "Json index present for melbourne.\n",
      "Json index present for goa.\n",
      "Json index present for paris.\n",
      "Json index present for trondheim.\n",
      "Json index present for bangkok.\n"
     ]
    }
   ],
   "source": [
    "# Pre-check for creating indicies\n",
    "def generateIndicies(replace=False):\n",
    "    if not replace:\n",
    "        for city_dir in os.listdir(DATASET_VAL):\n",
    "            if city_dir == \".DS_Store\": continue\n",
    "            # get the files and check if query.json and database.json are in there\n",
    "            if all(elem in os.listdir(os.path.join(DATASET_VAL, city_dir)) for elem in ['query.json', 'database.json']):\n",
    "                print(f\"Json index present for {city_dir}.\")\n",
    "                continue\n",
    "            else:\n",
    "                print(f\"Could not find json index for {city_dir}. Creating all label indicies.\")\n",
    "                break\n",
    "        return\n",
    "    print('Indexing the data...')\n",
    "    if \"--dataset\" not in sys.argv:\n",
    "        sys.argv = [sys.argv[0]]\n",
    "        sys.argv.extend([\"--dataset\", \"msls\", \"--root_dir\", os.path.join(os.getcwd(), DATASET_ROOT)])\n",
    "    runpy.run_module('create_json_idx', run_name='__main__')\n",
    "    print('Done indexing data.')\n",
    "\n",
    "\n",
    "# Generates indicies if needed\n",
    "generateIndicies(replace=RE_INDEX)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "234891b9-c702-470c-8068-e963f1021f38",
   "metadata": {},
   "source": [
    "----\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40511d9e-ccaf-4e6e-b6c4-029b67218d97",
   "metadata": {},
   "source": [
    "## Feature Extraction\n",
    "Section regarding local and global feature extraction depending on the necessity. The extracted features are saved in results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb380e7f-aad1-4151-b811-01bfe927daea",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Helper Functions Defintion\n",
    "All helper functions listed for feature extraction and other aspects of feature extraction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3500d4e-10be-402b-9ce7-b02ebc7d1d5e",
   "metadata": {},
   "source": [
    "##### Helper Function #1: Extract Features to File\n",
    "These functions extract the features of the msls dataset into the respective files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "0a3ba128-4f5b-4bab-9656-7e05cca17b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features_to_file(dataloader, model, f_length, feature_filepath, device):\n",
    "    if not os.path.exists(feature_filepath):\n",
    "        # pre-allocate memory for features\n",
    "        feats = torch.empty((len(dataloader.dataset), f_length), device=device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for i, batch in tqdm(enumerate(dataloader), desc=\"Extracting features for \"+feature_filepath):\n",
    "                x = model(batch.to(device)).squeeze(-1).squeeze(-1)\n",
    "                feats[i * dataloader.batch_size:i * dataloader.batch_size + dataloader.batch_size] = x\n",
    "\n",
    "        # move features to CPU and save to file\n",
    "        feats = feats.cpu().numpy()\n",
    "        np.save(feature_filepath, feats)\n",
    "        print(f\"{feature_filepath} has been saved.\")\n",
    "    else:\n",
    "        print(feature_filepath, \"already exists. Skipping.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b801914-b1ca-4b5c-9751-3e976d6b9375",
   "metadata": {},
   "source": [
    "##### Helper Function #2: Extract Features\n",
    "These functions extract the features of the msls dataset. Depending on the necessity, there are two helper functions, one for local feature extraction and another for global feature extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "a81b2fb4-d0fe-4ad9-a487-6e454a108693",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(model, feat_format, f_length, device):\n",
    "    # Get all the cities in the validation set\n",
    "    cities = default_cities[\"val\"]\n",
    "\n",
    "    # Create a transformation for each image in the dataloader\n",
    "    image_transform = ttf.Compose([\n",
    "        ttf.Resize(size=(IMG_SIZE[0], IMG_SIZE[1])),\n",
    "        ttf.ToTensor(),\n",
    "        ttf.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    # Loop through each city to create specific features for each\n",
    "    for city in cities:\n",
    "        print(city)\n",
    "        query_idx = os.path.join(DATASET_ROOT, \"train_val\", city, \"query.json\")\n",
    "        map_idx = os.path.join(DATASET_ROOT, \"train_val\", city, \"database.json\")\n",
    "\n",
    "        q_dataloader = create_dataloader(\"test\", DATASET_ROOT, query_idx, None, image_transform, BATCH_SIZE)\n",
    "        q_feature_filepath = os.path.join(RESULTS_ROOT, feat_format.format(city=city, db=\"query\"))\n",
    "        extract_features_to_file(q_dataloader, model, f_length, q_feature_filepath, device)\n",
    "\n",
    "        m_dataloader = create_dataloader(\"test\", DATASET_ROOT, map_idx, None, image_transform, BATCH_SIZE)\n",
    "        m_feature_filepath = os.path.join(RESULTS_ROOT, feat_format.format(city=city, db=\"map\"))\n",
    "        extract_features_to_file(m_dataloader, model, f_length, m_feature_filepath, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92536086-1421-45c8-8e0f-2f84900b0f62",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "173f3bf3-fb48-41ba-9911-81f038d3cbd9",
   "metadata": {},
   "source": [
    "#### Create the Model and Load Weights\n",
    "Create the model from the given backend and load the weights of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "071a2a11-f876-4023-aa41-45f98042a299",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/jovyan/.cache/torch/hub/facebookresearch_WSL-Images_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " the layers of the resnext101_32x8d_wsl are: odict_keys(['conv1', 'bn1', 'relu', 'maxpool', 'layer1', 'layer2', 'layer3', 'layer4', 'avgpool', 'fc'])\n",
      " the layers of the resnext101_32x8d_wsl are after removing the last two layers (avgpool and fc): odict_keys(['0', '1', '2', '3', '4', '5', '6', '7'])\n",
      "Number of layers: 8\n",
      "0 Conv2d IS TRAINED\n",
      "1 BatchNorm2d IS TRAINED\n",
      "2 ReLU IS TRAINED\n",
      "3 MaxPool2d IS TRAINED\n",
      "4 Sequential IS TRAINED\n",
      "5 Sequential IS TRAINED\n",
      "6 Sequential IS TRAINED\n",
      "7 Sequential IS TRAINED\n",
      "Model loaded!\n"
     ]
    }
   ],
   "source": [
    "# Create the models\n",
    "model = create_model(BACKBONE, POOL, norm=NORM, mode=\"single\").to(device)\n",
    "\n",
    "# Load the weights\n",
    "try:\n",
    "    model.load_state_dict(torch.load(MODEL_WEIGHTS)[\"model_state_dict\"])\n",
    "except:\n",
    "    model.load_state_dict(torch.load(MODEL_WEIGHTS)[\"state_dict\"])\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()\n",
    "\n",
    "# Set model to inference mode\n",
    "model.eval()\n",
    "print('Model loaded!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "336e8764-4105-4964-9d15-38228fd91bb1",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45239fb6-b933-47ee-a49e-3ea304725420",
   "metadata": {},
   "source": [
    "#### Generate Features\n",
    "Function to generate features and create the individual models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "c8904c71-11d9-4111-9603-8153c5fa3ede",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cph\n",
      "/home/jovyan/APFIR/generalized_contrastive_loss/results/MSLS/val/MSLS_resnext_GeM_480_GCL_cph_dilation_L2_local_queryfeats.npy already exists. Skipping.\n",
      "/home/jovyan/APFIR/generalized_contrastive_loss/results/MSLS/val/MSLS_resnext_GeM_480_GCL_cph_dilation_L2_local_mapfeats.npy already exists. Skipping.\n",
      "sf\n",
      "/home/jovyan/APFIR/generalized_contrastive_loss/results/MSLS/val/MSLS_resnext_GeM_480_GCL_sf_dilation_L2_local_queryfeats.npy already exists. Skipping.\n",
      "/home/jovyan/APFIR/generalized_contrastive_loss/results/MSLS/val/MSLS_resnext_GeM_480_GCL_sf_dilation_L2_local_mapfeats.npy already exists. Skipping.\n",
      "cph\n",
      "/home/jovyan/APFIR/generalized_contrastive_loss/results/MSLS/val/MSLS_resnext_GeM_480_GCL_cph_L2_global_queryfeats.npy already exists. Skipping.\n",
      "/home/jovyan/APFIR/generalized_contrastive_loss/results/MSLS/val/MSLS_resnext_GeM_480_GCL_cph_L2_global_mapfeats.npy already exists. Skipping.\n",
      "sf\n",
      "/home/jovyan/APFIR/generalized_contrastive_loss/results/MSLS/val/MSLS_resnext_GeM_480_GCL_sf_L2_global_queryfeats.npy already exists. Skipping.\n",
      "/home/jovyan/APFIR/generalized_contrastive_loss/results/MSLS/val/MSLS_resnext_GeM_480_GCL_sf_L2_global_mapfeats.npy already exists. Skipping.\n",
      "SUMMARY OF LOCAL FEATURE MODEL:\n",
      "=================================================================================================================================================\n",
      "Layer (type:depth-idx)                        Input Shape               Output Shape              Param #                   Trainable\n",
      "=================================================================================================================================================\n",
      "Sequential                                    [1, 3, 480, 640]          [1, 2048, 1, 1]           --                        Partial\n",
      "├─Conv2d: 1-1                                 [1, 3, 480, 640]          [1, 64, 240, 320]         9,408                     True\n",
      "├─BatchNorm2d: 1-2                            [1, 64, 240, 320]         [1, 64, 240, 320]         128                       True\n",
      "├─ReLU: 1-3                                   [1, 64, 240, 320]         [1, 64, 240, 320]         --                        --\n",
      "├─MaxPool2d: 1-4                              [1, 64, 240, 320]         [1, 64, 120, 160]         --                        --\n",
      "├─Sequential: 1-5                             [1, 64, 120, 160]         [1, 256, 120, 160]        --                        True\n",
      "│    └─Bottleneck: 2-1                        [1, 64, 120, 160]         [1, 256, 120, 160]        118,784                   True\n",
      "│    └─Bottleneck: 2-2                        [1, 256, 120, 160]        [1, 256, 120, 160]        151,040                   True\n",
      "│    └─Bottleneck: 2-3                        [1, 256, 120, 160]        [1, 256, 120, 160]        151,040                   True\n",
      "├─Sequential: 1-6                             [1, 256, 120, 160]        [1, 512, 60, 80]          --                        True\n",
      "│    └─Bottleneck: 2-4                        [1, 256, 120, 160]        [1, 512, 60, 80]          602,112                   True\n",
      "│    └─Bottleneck: 2-5                        [1, 512, 60, 80]          [1, 512, 60, 80]          601,088                   True\n",
      "│    └─Bottleneck: 2-6                        [1, 512, 60, 80]          [1, 512, 60, 80]          601,088                   True\n",
      "│    └─Bottleneck: 2-7                        [1, 512, 60, 80]          [1, 512, 60, 80]          601,088                   True\n",
      "├─Sequential: 1-7                             [1, 512, 60, 80]          [1, 1024, 30, 40]         --                        True\n",
      "│    └─Bottleneck: 2-8                        [1, 512, 60, 80]          [1, 1024, 30, 40]         2,400,256                 True\n",
      "│    └─Bottleneck: 2-9                        [1, 1024, 30, 40]         [1, 1024, 30, 40]         2,398,208                 True\n",
      "│    └─Bottleneck: 2-10                       [1, 1024, 30, 40]         [1, 1024, 30, 40]         2,398,208                 True\n",
      "│    └─Bottleneck: 2-11                       [1, 1024, 30, 40]         [1, 1024, 30, 40]         2,398,208                 True\n",
      "│    └─Bottleneck: 2-12                       [1, 1024, 30, 40]         [1, 1024, 30, 40]         2,398,208                 True\n",
      "│    └─Bottleneck: 2-13                       [1, 1024, 30, 40]         [1, 1024, 30, 40]         2,398,208                 True\n",
      "│    └─Bottleneck: 2-14                       [1, 1024, 30, 40]         [1, 1024, 30, 40]         2,398,208                 True\n",
      "│    └─Bottleneck: 2-15                       [1, 1024, 30, 40]         [1, 1024, 30, 40]         2,398,208                 True\n",
      "│    └─Bottleneck: 2-16                       [1, 1024, 30, 40]         [1, 1024, 30, 40]         2,398,208                 True\n",
      "│    └─Bottleneck: 2-17                       [1, 1024, 30, 40]         [1, 1024, 30, 40]         2,398,208                 True\n",
      "│    └─Bottleneck: 2-18                       [1, 1024, 30, 40]         [1, 1024, 30, 40]         2,398,208                 True\n",
      "│    └─Bottleneck: 2-19                       [1, 1024, 30, 40]         [1, 1024, 30, 40]         2,398,208                 True\n",
      "│    └─Bottleneck: 2-20                       [1, 1024, 30, 40]         [1, 1024, 30, 40]         2,398,208                 True\n",
      "│    └─Bottleneck: 2-21                       [1, 1024, 30, 40]         [1, 1024, 30, 40]         2,398,208                 True\n",
      "│    └─Bottleneck: 2-22                       [1, 1024, 30, 40]         [1, 1024, 30, 40]         2,398,208                 True\n",
      "│    └─Bottleneck: 2-23                       [1, 1024, 30, 40]         [1, 1024, 30, 40]         2,398,208                 True\n",
      "│    └─Bottleneck: 2-24                       [1, 1024, 30, 40]         [1, 1024, 30, 40]         2,398,208                 True\n",
      "│    └─Bottleneck: 2-25                       [1, 1024, 30, 40]         [1, 1024, 30, 40]         2,398,208                 True\n",
      "│    └─Bottleneck: 2-26                       [1, 1024, 30, 40]         [1, 1024, 30, 40]         2,398,208                 True\n",
      "│    └─Bottleneck: 2-27                       [1, 1024, 30, 40]         [1, 1024, 30, 40]         2,398,208                 True\n",
      "│    └─Bottleneck: 2-28                       [1, 1024, 30, 40]         [1, 1024, 30, 40]         2,398,208                 True\n",
      "│    └─Bottleneck: 2-29                       [1, 1024, 30, 40]         [1, 1024, 30, 40]         2,398,208                 True\n",
      "│    └─Bottleneck: 2-30                       [1, 1024, 30, 40]         [1, 1024, 30, 40]         2,398,208                 True\n",
      "├─DilatedConv: 1-8                            [1, 1024, 30, 40]         [1, 2048, 30, 40]         --                        True\n",
      "│    └─ModuleList: 2-31                       --                        --                        14,682,112                True\n",
      "├─Conv2d: 1-9                                 [1, 2048, 30, 40]         [1, 2048, 30, 40]         4,196,352                 True\n",
      "├─Conv2d: 1-10                                [1, 2048, 30, 40]         [1, 2048, 30, 40]         4,194,304                 True\n",
      "├─Conv2d: 1-11                                [1, 2048, 30, 40]         [1, 2048, 30, 40]         4,196,352                 True\n",
      "├─ReLU: 1-12                                  [1, 2048, 30, 40]         [1, 2048, 30, 40]         --                        --\n",
      "├─BatchNorm2d: 1-13                           [1, 2048, 30, 40]         [1, 2048, 30, 40]         4,096                     True\n",
      "├─Softplus: 1-14                              [1, 2048, 30, 40]         [1, 2048, 30, 40]         --                        --\n",
      "├─GeM: 1-15                                   [1, 2048, 30, 40]         [1, 2048, 1, 1]           (1)                       False\n",
      "=================================================================================================================================================\n",
      "Total params: 85,269,825\n",
      "Trainable params: 85,269,824\n",
      "Non-trainable params: 1\n",
      "Total mult-adds (G): 122.08\n",
      "=================================================================================================================================================\n",
      "Input size (MB): 3.69\n",
      "Forward/backward pass size (MB): 3022.87\n",
      "Params size (MB): 341.08\n",
      "Estimated Total Size (MB): 3367.63\n",
      "=================================================================================================================================================\n",
      "\n",
      "\n",
      "SUMMARY OF GLOBAL FEATURE MODEL:\n",
      "============================================================================================================================================\n",
      "Layer (type:depth-idx)                   Input Shape               Output Shape              Param #                   Trainable\n",
      "============================================================================================================================================\n",
      "Sequential                               [1, 3, 480, 640]          [1, 2048, 1, 1]           --                        Partial\n",
      "├─Conv2d: 1-1                            [1, 3, 480, 640]          [1, 64, 240, 320]         9,408                     True\n",
      "├─BatchNorm2d: 1-2                       [1, 64, 240, 320]         [1, 64, 240, 320]         128                       True\n",
      "├─ReLU: 1-3                              [1, 64, 240, 320]         [1, 64, 240, 320]         --                        --\n",
      "├─MaxPool2d: 1-4                         [1, 64, 240, 320]         [1, 64, 120, 160]         --                        --\n",
      "├─Sequential: 1-5                        [1, 64, 120, 160]         [1, 256, 120, 160]        --                        True\n",
      "│    └─Bottleneck: 2-1                   [1, 64, 120, 160]         [1, 256, 120, 160]        118,784                   True\n",
      "│    └─Bottleneck: 2-2                   [1, 256, 120, 160]        [1, 256, 120, 160]        151,040                   True\n",
      "│    └─Bottleneck: 2-3                   [1, 256, 120, 160]        [1, 256, 120, 160]        151,040                   True\n",
      "├─Sequential: 1-6                        [1, 256, 120, 160]        [1, 512, 60, 80]          --                        True\n",
      "│    └─Bottleneck: 2-4                   [1, 256, 120, 160]        [1, 512, 60, 80]          602,112                   True\n",
      "│    └─Bottleneck: 2-5                   [1, 512, 60, 80]          [1, 512, 60, 80]          601,088                   True\n",
      "│    └─Bottleneck: 2-6                   [1, 512, 60, 80]          [1, 512, 60, 80]          601,088                   True\n",
      "│    └─Bottleneck: 2-7                   [1, 512, 60, 80]          [1, 512, 60, 80]          601,088                   True\n",
      "├─Sequential: 1-7                        [1, 512, 60, 80]          [1, 1024, 30, 40]         --                        True\n",
      "│    └─Bottleneck: 2-8                   [1, 512, 60, 80]          [1, 1024, 30, 40]         2,400,256                 True\n",
      "│    └─Bottleneck: 2-9                   [1, 1024, 30, 40]         [1, 1024, 30, 40]         2,398,208                 True\n",
      "│    └─Bottleneck: 2-10                  [1, 1024, 30, 40]         [1, 1024, 30, 40]         2,398,208                 True\n",
      "│    └─Bottleneck: 2-11                  [1, 1024, 30, 40]         [1, 1024, 30, 40]         2,398,208                 True\n",
      "│    └─Bottleneck: 2-12                  [1, 1024, 30, 40]         [1, 1024, 30, 40]         2,398,208                 True\n",
      "│    └─Bottleneck: 2-13                  [1, 1024, 30, 40]         [1, 1024, 30, 40]         2,398,208                 True\n",
      "│    └─Bottleneck: 2-14                  [1, 1024, 30, 40]         [1, 1024, 30, 40]         2,398,208                 True\n",
      "│    └─Bottleneck: 2-15                  [1, 1024, 30, 40]         [1, 1024, 30, 40]         2,398,208                 True\n",
      "│    └─Bottleneck: 2-16                  [1, 1024, 30, 40]         [1, 1024, 30, 40]         2,398,208                 True\n",
      "│    └─Bottleneck: 2-17                  [1, 1024, 30, 40]         [1, 1024, 30, 40]         2,398,208                 True\n",
      "│    └─Bottleneck: 2-18                  [1, 1024, 30, 40]         [1, 1024, 30, 40]         2,398,208                 True\n",
      "│    └─Bottleneck: 2-19                  [1, 1024, 30, 40]         [1, 1024, 30, 40]         2,398,208                 True\n",
      "│    └─Bottleneck: 2-20                  [1, 1024, 30, 40]         [1, 1024, 30, 40]         2,398,208                 True\n",
      "│    └─Bottleneck: 2-21                  [1, 1024, 30, 40]         [1, 1024, 30, 40]         2,398,208                 True\n",
      "│    └─Bottleneck: 2-22                  [1, 1024, 30, 40]         [1, 1024, 30, 40]         2,398,208                 True\n",
      "│    └─Bottleneck: 2-23                  [1, 1024, 30, 40]         [1, 1024, 30, 40]         2,398,208                 True\n",
      "│    └─Bottleneck: 2-24                  [1, 1024, 30, 40]         [1, 1024, 30, 40]         2,398,208                 True\n",
      "│    └─Bottleneck: 2-25                  [1, 1024, 30, 40]         [1, 1024, 30, 40]         2,398,208                 True\n",
      "│    └─Bottleneck: 2-26                  [1, 1024, 30, 40]         [1, 1024, 30, 40]         2,398,208                 True\n",
      "│    └─Bottleneck: 2-27                  [1, 1024, 30, 40]         [1, 1024, 30, 40]         2,398,208                 True\n",
      "│    └─Bottleneck: 2-28                  [1, 1024, 30, 40]         [1, 1024, 30, 40]         2,398,208                 True\n",
      "│    └─Bottleneck: 2-29                  [1, 1024, 30, 40]         [1, 1024, 30, 40]         2,398,208                 True\n",
      "│    └─Bottleneck: 2-30                  [1, 1024, 30, 40]         [1, 1024, 30, 40]         2,398,208                 True\n",
      "├─Sequential: 1-8                        [1, 1024, 30, 40]         [1, 2048, 15, 20]         --                        True\n",
      "│    └─Bottleneck: 2-31                  [1, 1024, 30, 40]         [1, 2048, 15, 20]         9,584,640                 True\n",
      "│    └─Bottleneck: 2-32                  [1, 2048, 15, 20]         [1, 2048, 15, 20]         9,580,544                 True\n",
      "│    └─Bottleneck: 2-33                  [1, 2048, 15, 20]         [1, 2048, 15, 20]         9,580,544                 True\n",
      "├─GeM: 1-9                               [1, 2048, 15, 20]         [1, 2048, 1, 1]           (1)                       False\n",
      "============================================================================================================================================\n",
      "Total params: 86,742,337\n",
      "Trainable params: 86,742,336\n",
      "Non-trainable params: 1\n",
      "Total mult-adds (G): 100.48\n",
      "============================================================================================================================================\n",
      "Input size (MB): 3.69\n",
      "Forward/backward pass size (MB): 3057.27\n",
      "Params size (MB): 346.97\n",
      "Estimated Total Size (MB): 3407.93\n",
      "============================================================================================================================================\n"
     ]
    }
   ],
   "source": [
    "def generateFeatures(global_feat_model, input_size, hidden_dim, output_dim,\n",
    "                     feature_types, norm, dilation, generate_summary, device,\n",
    "                     globalfeat_format, localfeat_format):\n",
    "    # Create the local feature model by removing the last layer and the pooling\n",
    "    # local_feat_model = torch.nn.Sequential(*(list(list(global_feat_model.children())[0].children ())[:-1]), list(global_feat_model.children())[1])\n",
    "    local_feat_model = torch.nn.Sequential(*(list(list(global_feat_model.children())[0].children())[:-1]))\n",
    "\n",
    "    # global_feat_model = torch.nn.Sequential(*(list(local_feat_model.children())), list(list(global_feat_model.children())[0].children ())[-1], list(global_feat_model.children())[1])\n",
    "    global_feat_model = torch.nn.Sequential(*(list(list(global_feat_model.children())[0].children())), list(global_feat_model.children())[1]).to(device)\n",
    "\n",
    "    # Create the local branch and add the layers together\n",
    "    local_branch = LocalBranch(input_dim=hidden_dim, out_channel=output_dim, norm=norm, dilation=dilation, image_size=IMG_SIZE)\n",
    "    local_branch.cuda() # move the local branch to cuda as well\n",
    "    local_feat_model = torch.nn.Sequential(*(list(local_feat_model.children())), *(list(local_branch.children()))).to(device)\n",
    "\n",
    "    # Columns to show for model summary\n",
    "    cols = (\"input_size\", \"output_size\", \"num_params\", \"trainable\")\n",
    "\n",
    "    if not os.path.exists(RESULTS_ROOT):\n",
    "        os.makedirs(RESULTS_ROOT)\n",
    "\n",
    "    if feature_types == \"both\":\n",
    "        # generate both features\n",
    "        # first, generate local features\n",
    "        extract_features(\n",
    "            global_feat_model,\n",
    "            feat_format=localfeat_format,\n",
    "            f_length=output_dim,\n",
    "            device=device\n",
    "        )\n",
    "        # then, generate global features\n",
    "        extract_features(\n",
    "            global_feat_model,\n",
    "            feat_format=globalfeat_format,\n",
    "            f_length=output_dim,\n",
    "            device=device\n",
    "        )\n",
    "        if generate_summary:\n",
    "            print(\"SUMMARY OF LOCAL FEATURE MODEL:\")\n",
    "            local_model_stats = summary(local_feat_model, input_size = input_size, col_names = cols, verbose = 1, depth=2)\n",
    "            # Save the model summary as an image\n",
    "            text2png(lines=str(local_model_stats).split(\"\\n\")[1:],\n",
    "                     fullpath=os.path.join(MODEL_IMGS, model_format.format(ftype=\"local\")),\n",
    "                     fontfullpath=FONT_PATH,\n",
    "                     fontsize=13,\n",
    "                     leftpadding=5,\n",
    "                     rightpadding=5,\n",
    "                     width=250*len(cols)\n",
    "                    )\n",
    "\n",
    "            print(\"\\n\\nSUMMARY OF GLOBAL FEATURE MODEL:\")\n",
    "            global_model_stats = summary(global_feat_model, input_size = input_size, col_names = cols, verbose = 1, depth=2)\n",
    "            # Save the model summary as an image\n",
    "            text2png(lines=str(global_model_stats).split(\"\\n\")[1:],\n",
    "                     fullpath=os.path.join(MODEL_IMGS, model_format.format(ftype=\"global\").replace(\"_dilation\", \"\")),\n",
    "                     fontfullpath=FONT_PATH,\n",
    "                     fontsize=13,\n",
    "                     leftpadding=5,\n",
    "                     rightpadding=5,\n",
    "                     width=250*len(cols)\n",
    "                    )\n",
    "    elif feature_types == \"global\":\n",
    "        # generate only global features\n",
    "        extract_features(\n",
    "            global_feat_model,\n",
    "            feat_format=globalfeat_format,\n",
    "            f_length=output_dim,\n",
    "            device=device\n",
    "        )\n",
    "        if generate_summary:\n",
    "            print(\"SUMMARY OF LOCAL FEATURE MODEL:\")\n",
    "            local_model_stats = summary(global_feat_model, input_size = input_size, col_names = cols, verbose = 1, depth=2)\n",
    "            # Save the model summary as an image\n",
    "            text2png(lines=str(local_model_stats).split(\"\\n\")[1:],\n",
    "                     fullpath=os.path.join(MODEL_IMGS, model_format.format(ftype=\"local\")),\n",
    "                     fontfullpath=FONT_PATH,\n",
    "                     fontsize=13,\n",
    "                     leftpadding=5,\n",
    "                     rightpadding=5,\n",
    "                     width=250*len(cols)\n",
    "                    )\n",
    "    else:\n",
    "        # generate only local features\n",
    "        extract_features(\n",
    "            global_feat_model,\n",
    "            feat_format=localfeat_format,\n",
    "            f_length=output_dim,\n",
    "            device=device\n",
    "        )\n",
    "        if generate_summary:\n",
    "            print(\"SUMMARY OF LOCAL FEATURE MODEL:\")\n",
    "            global_model_stats = summary(local_feat_model, input_size = input_size, col_names = cols, verbose = 1, depth=2)\n",
    "            # Save the model summary as an image\n",
    "            text2png(lines=str(global_model_stats).split(\"\\n\")[1:],\n",
    "                     fullpath=os.path.join(MODEL_IMGS, model_format.format(ftype=\"global\").replace(\"_dilation\", \"\")),\n",
    "                     fontfullpath=FONT_PATH,\n",
    "                     fontsize=13,\n",
    "                     leftpadding=5,\n",
    "                     rightpadding=5,\n",
    "                     width=250*len(cols)\n",
    "                    )\n",
    "\n",
    "\n",
    "generateFeatures(\n",
    "    model,\n",
    "    input_size=(BATCH_SIZE, 3, 480, 640),\n",
    "    hidden_dim=1024,\n",
    "    output_dim=2048,\n",
    "    feature_types=\"both\",\n",
    "    norm=NORM,\n",
    "    dilation=DILATION,\n",
    "    globalfeat_format=global_feat_format,\n",
    "    localfeat_format=local_feat_format,\n",
    "    generate_summary=True,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92864953-6584-4196-920c-3ca616064284",
   "metadata": {},
   "source": [
    "----\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e8742f-1875-40d9-a92f-3497660bfbbd",
   "metadata": {},
   "source": [
    "## Re-Ranking\n",
    "Particular section dedicated to fusion and re-ranking methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "444dfca4-db4e-43e4-b3c8-e853c879fb56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the rerank notebook\n",
    "%run rerank.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93be858a-9a6d-4bbb-8c5d-719c5a0fc522",
   "metadata": {},
   "source": [
    "### Fusion\n",
    "First, we define the fusion methods for the re-ranking part."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7546122-efa4-4d93-b55b-3dcbe08b6f1d",
   "metadata": {},
   "source": [
    "##### Orthogonal Fusion\n",
    "A method of fusion that uses vector projection to project local features onto the global features and then concatenates them. (Written as a nn module fully-connected layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "8c769b48-545f-4afc-96f0-21fda15cb192",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OrthogonalFusion(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, local_feat, global_feat):\n",
    "        global_feat_norm = torch.norm(global_feat, p=2, dim=1, keepdim=True)\n",
    "        projection = torch.bmm(global_feat.unsqueeze(1), torch.flatten(local_feat, start_dim=1).unsqueeze(2))\n",
    "        projection = torch.bmm(global_feat.unsqueeze(2), projection).view(local_feat.size())\n",
    "        projection = projection / (global_feat_norm * global_feat_norm)\n",
    "        orthogonal_comp = local_feat - projection\n",
    "        return torch.cat([global_feat, orthogonal_comp], dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9657eaa3-9959-4484-a362-86d541302763",
   "metadata": {},
   "source": [
    "##### Hadamard Fusion\n",
    "A method of fusion that simply does a Hadamard vector multiplication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "e30aa370-ac5b-40e5-b7d9-1c6d092f9219",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hadamard_fusion(global_features, local_features):\n",
    "    \"\"\"when you apply just the sequeeze() it removes all the ones\n",
    "    #so (1,2048,1,1) after squeezing -> (2048), adding that extra dimension on the zero \n",
    "    #axis :- unsqueeze(0) (1,2048)\"\"\"\n",
    "    if global_features.shape[0] == 1:\n",
    "        x = np.multiply(local_features, global_features).squeeze().unsqueeze(0)\n",
    "    else:\n",
    "        # if we have batch size!=1, then we dont have to unsqueeze because\n",
    "        # it wont squeeze the batch size hadamard product\n",
    "        x = np.multiply(local_features, global_features).squeeze()\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe0c3a0f-aa91-4852-93d4-9d5bfbeadc14",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7aff486-63a5-4796-8d2d-3d3c3442e5fa",
   "metadata": {},
   "source": [
    "### Re-Ranking\n",
    "Setting up configuration and re-ranking the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "775bb353-2532-414d-9cc4-d263546358a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration to be passed for re-ranking\n",
    "configuration = {\n",
    "    \"cities\": [\"cph\", \"sf\"],\n",
    "    \"prediction_path\": PREDICTION_PATH,\n",
    "    \"features_path\": RESULTS_ROOT,\n",
    "    \"local_feat_format\": local_feat_format,\n",
    "    \"global_feat_format\": global_feat_format,\n",
    "    \"results_file_format\": results_format,\n",
    "    \"fusion\": FUSION,\n",
    "    \"data_path\": DATASET_VAL,\n",
    "    \"similarity_measure\": torch.cdist,\n",
    "    \"fusion_method\": OrthogonalFusion()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "bfd55599-096e-4459-ad1b-304b1b2cdff4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ranking for cph: 100%|██████████| 6595/6595 [00:06<00:00, 1008.79it/s]\n",
      "Ranking for sf: 100%|██████████| 4525/4525 [00:04<00:00, 929.92it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing the results to /home/jovyan/APFIR/generalized_contrastive_loss/results/MSLS/val/MSLS_resnext_GeM_480_GCL_dilation_L2_results.txt.\n",
      "CPU times: user 1min 1s, sys: 4.83 s, total: 1min 6s\n",
      "Wall time: 12.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Re-rank and return the dataframe and the result file location\n",
    "reranked_df, saved_results_file = rank(configuration)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a241013-0da4-4fef-9f34-e4a94d7fc1e6",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Evaluation\n",
    "Use the MSLS evaluation server to evaluate the results of the re-ranking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "a5061f9a-297c-4d55-b586-5ddd993e9d9c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running evaluation...\n",
      "Finished running evaluation.\n"
     ]
    }
   ],
   "source": [
    "# Evaluation command with required paths\n",
    "command = [\"python\", os.path.join(os.getcwd(), 'generalized_contrastive_loss', 'mapillary_sls', 'evaluate.py'), \"--prediction\", saved_results_file, \"--msls-root\", DATASET_ROOT, \"--cities\", \"cph,sf\"]\n",
    "print(\"Running evaluation...\")\n",
    "# Run the evaluation and wait for completion\n",
    "result = subprocess.run(command, capture_output=True, text=True)\n",
    "print(\"Finished running evaluation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "5150b059-1621-4930-bffd-f47f80bce5e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the strings to be printed out for evaluation information\n",
    "\n",
    "eval_result = []\n",
    "# Add information about the parameters\n",
    "eval_result.append(f\"FUSION: {FUSION}\")  # Whether it is fusion or not\n",
    "eval_result.append(f\"BACKBONE: {BACKBONE}\")  # backbone for the model\n",
    "eval_result.append(f\"POOL: {POOL}\")  # pooling type\n",
    "eval_result.append(f\"NORM: {NORM}\")  # norm type\n",
    "eval_result.append(f\"DILATION: {DILATION}\")  # Whether to add dilation layer\n",
    "eval_result.append(\"\")  # Empty line for asthetics\n",
    "\n",
    "# Get the eval results only\n",
    "eval_result.extend(result.stdout.split('\\n')[-9:-1])\n",
    "\n",
    "# Save the result as an image\n",
    "text2png(lines=eval_result,\n",
    "         fullpath=os.path.join(RESULT_IMGS, results_format.replace(\".txt\", \".png\")),\n",
    "         fontfullpath=FONT_PATH,\n",
    "         fontsize=13,\n",
    "         leftpadding=5,\n",
    "         rightpadding=5\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d734978-06b4-40d2-adce-b5de95b2255b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyheat import PyHeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43cd37db-e721-4507-93f1-48211052dba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ph = PyHeat(\"test.py\")\n",
    "# ph.create_heatmap()\n",
    "# mytest.main([\"-x\",\"7\",\"-y\",\"6\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f3ae6b-f24b-4351-8c1d-90d63148db36",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "apfir",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13 | packaged by conda-forge | (default, Mar 25 2022, 06:05:16) \n[Clang 12.0.1 ]"
  },
  "vscode": {
   "interpreter": {
    "hash": "545453764902d2d334f43e52c32418f23b05e541ad8c155b39a8e06721c060bf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
