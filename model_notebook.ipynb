{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2e3d421-4b5d-407a-b330-0e02e5593fc9",
   "metadata": {},
   "source": [
    "# Imports and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d2c9cf8-7f2c-4913-87ba-918f5788e44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from generalized_contrastive_loss.datasets import *\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6147a94a-4331-482c-ae89-b162327ce8a9",
   "metadata": {},
   "source": [
    "# Models and DataLoaders\n",
    "Both the Siamese network and Resnext base network is defined here. The definition of the BaseNet is the same as the Siamese network trained with GCL. The general dataloader is also defined here to be used for feature extraction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd9b3ed-65ed-4a73-b107-3f2a98181ef6",
   "metadata": {},
   "source": [
    "#### BaseNet and Siamese Network\n",
    "Definition of GeM pooling, the BaseNet and Siamese Network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a9b55db1-2946-4ada-9c4c-a8f4a8425e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeM(nn.Module):\n",
    "    def __init__(self, p=3, eps=1e-6, requires_grad=False):\n",
    "        super(GeM, self).__init__()\n",
    "        self.p = nn.Parameter(torch.ones(1)*p, requires_grad=requires_grad)\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.gem(x, p=self.p, eps=self.eps)\n",
    "\n",
    "    def gem(self, x, p=3, eps=1e-6):\n",
    "        return F.avg_pool2d(x.clamp(min=eps).pow(p), (x.size(-2), x.size(-1))).pow(1./p)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + '(' + 'p=' + '{:.4f}'.format(self.p.data.tolist()[0]) + ', ' + 'eps=' + str(self.eps) + ')'\n",
    "\n",
    "\n",
    "class BaseNet(nn.Module):\n",
    "    def __init__(self, backbone, global_pool=None, poolkernel=7, norm=None, p=3, num_clusters=64):\n",
    "        super(BaseNet, self).__init__()\n",
    "        self.backbone = backbone\n",
    "        for name, param in self.backbone.named_parameters():\n",
    "            n = param.size()[0]\n",
    "        self.num_features = n\n",
    "        self.pretrained_cfg = {}\n",
    "        self.num_classes = 0\n",
    "        if global_pool == \"max\":\n",
    "            self.pool = nn.AdaptiveMaxPool2d(output_size=(1, 1))\n",
    "        elif global_pool == \"avg\":\n",
    "            self.pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))\n",
    "        elif global_pool == \"GeM\":\n",
    "            self.pool = GeM(p=p)\n",
    "        else:\n",
    "            self.pool = None\n",
    "        self.norm = norm\n",
    "\n",
    "    def forward(self, x0):\n",
    "        out = self.backbone.forward(x0)\n",
    "        out = self.pool.forward(out)\n",
    "\n",
    "        if self.norm == \"L2\":\n",
    "            out=nn.functional.normalize(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class SiameseNet(BaseNet):\n",
    "    def __init__(self, backbone, global_pool=None, poolkernel=7,norm=None, p=3,num_clusters=64):\n",
    "        super(SiameseNet, self).__init__(backbone, global_pool, poolkernel, norm=norm, p=p,num_clusters=num_clusters)\n",
    "\n",
    "    def forward(self, x0, x1):\n",
    "        out0 = super(SiameseNet, self).forward(x0)\n",
    "        out1 = super(SiameseNet, self).forward(x1)\n",
    "        return out0, out1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b388ca4f-0d34-4157-a3b5-42bcac2fc5ab",
   "metadata": {},
   "source": [
    "#### DataLoader\n",
    "Creates a dataloader and the backbone for the model. Depending on the type of dataset, the network is initialized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5f396d8-af29-4a04-a795-71a77f601ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader(dataset, root_dir, idx_file, gt_file, image_t, batch_size):\n",
    "    # Create dataset\n",
    "    if dataset==\"test\":\n",
    "        ds = TestDataSet(root_dir, idx_file, transform=image_t)\n",
    "        return DataLoader(ds, batch_size=batch_size, num_workers=4)\n",
    "\n",
    "    if dataset == \"soft_siamese\":\n",
    "        ds = SiameseDataSet(root_dir, idx_file, gt_file, ds_key=\"fov\", transform=image_t)\n",
    "    elif dataset == \"binary_siamese\":\n",
    "        ds = SiameseDataSet(root_dir, idx_file, gt_file, ds_key=\"sim\", transform=image_t)\n",
    "    return DataLoader(ds, batch_size=batch_size, num_workers=4, shuffle=True)\n",
    "\n",
    "\n",
    "def get_backbone(name):\n",
    "    if name == \"resnet18\":\n",
    "        backbone = models.resnet18(pretrained=True)\n",
    "    elif name == \"resnet34\":\n",
    "        backbone = models.resnet34(pretrained=True)\n",
    "    elif name == \"resnet152\":\n",
    "        backbone = models.resnet152(pretrained=True)\n",
    "    elif name == \"resnet50\":\n",
    "        backbone = models.resnet50(pretrained=True)\n",
    "    if name == \"densenet161\":\n",
    "        backbone = models.densenet161(pretrained=True).features\n",
    "        output_dim=2208\n",
    "    elif name == \"densenet121\":\n",
    "        backbone = models.densenet121(pretrained=True).features\n",
    "        output_dim=2208\n",
    "    elif name == \"vgg16\":\n",
    "        backbone = models.vgg16(pretrained=True).features\n",
    "        output_dim=512\n",
    "    elif name == \"resnext\":\n",
    "        backbone = torch.hub.load('facebookresearch/WSL-Images', 'resnext101_32x8d_wsl')\n",
    "        # Supposed to be ['conv1', 'bn1', 'relu', 'maxpool', 'layer1', 'layer2', 'layer3', 'layer4', 'avgpool', 'fc']\n",
    "        print(f\" the layers of the resnext101_32x8d_wsl are: {backbone._modules.keys()}\")\n",
    "    if \"resne\" in name:\n",
    "        backbone = torch.nn.Sequential(*(list(backbone.children())[:-2]))\n",
    "        output_dim = 2048\n",
    "        print(f\" the layers of the resnext101_32x8d_wsl are after removing the last two layers (avgpool and fc): {backbone._modules.keys()}\")\n",
    "    return backbone, output_dim\n",
    "\n",
    "\n",
    "def create_model(name, pool, last_layer=None, norm=None, p_gem=3, num_clusters=64, mode=\"siamese\"):\n",
    "    backbone, output_dim = get_backbone(name)\n",
    "    layers = len(list(backbone.children()))\n",
    "    print(f\"Number of layers: {layers}\")\n",
    "\n",
    "    if last_layer is None:\n",
    "        last_layer = layers\n",
    "    elif \"densenet\" in name:\n",
    "        last_layer=last_layer*2\n",
    "    elif \"vgg\" in name:\n",
    "        last_layer=last_layer*8-2\n",
    "    aux = 0\n",
    "    for c in backbone.children():\n",
    "        if aux < layers - last_layer:\n",
    "            print(aux, c._get_name(), \"IS FROZEN\")\n",
    "            for p in c.parameters():\n",
    "                p.requires_grad = False\n",
    "        else:\n",
    "            print(aux, c._get_name(), \"IS TRAINED\")\n",
    "        aux += 1\n",
    "    if mode==\"siamese\":\n",
    "        return SiameseNet(backbone, pool, norm=norm, p=p_gem, num_clusters=num_clusters)\n",
    "    elif mode==\"triplet\":\n",
    "        return TripletNet(backbone, pool, norm=norm, p=p_gem, num_clusters=num_clusters)\n",
    "    else:\n",
    "        return BaseNet(backbone, pool, norm=norm, p=p_gem)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b29506f-b235-4c6f-bfa4-e3d6140b2230",
   "metadata": {},
   "source": [
    "#### Local branch model\n",
    "> Dilation + Attention maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed89060-2ae4-49a9-a68d-1619959b1c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defined Dilation module which has dilation rates of 6, 12, 18 by default\n",
    "class DilatedConv(nn.Module):\n",
    "    def __init__(self, in_channel, out_channel, size, dilation_rates=[6, 12, 18]):\n",
    "        super(DilatedConv, self).__init__()\n",
    "        # Create dilated convolution for each rates\n",
    "        # Each conv: (1024 -> 512) * 3 rates = 1536\n",
    "        self.dilated_convs = [\n",
    "            nn.Conv2d(in_channel, int(out_channel/4),\n",
    "                      kernel_size=3, dilation=rate, padding=rate)\n",
    "            for rate in dilation_rates\n",
    "        ]\n",
    "        # GAP branch: (1024 -> 512) = 512\n",
    "        self.gap_branch = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Conv2d(in_channel, int(out_channel/4), kernel_size=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Upsample(size=size, mode='bilinear')\n",
    "        )\n",
    "        self.dilated_convs.append(self.gap_branch)\n",
    "        # Add all to the list of dilated convolutions\n",
    "        self.dilated_convs = nn.ModuleList(self.dilated_convs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward the data through each dilated convs\n",
    "        local_feat = []\n",
    "        for dilated_conv in self.dilated_convs:\n",
    "            local_feat.append(dilated_conv(x))\n",
    "        # Concatinate all features: (512 * 4) = 2048\n",
    "        local_feat = torch.cat(local_feat, dim=1)\n",
    "        return local_feat\n",
    "\n",
    "\n",
    "class LocalBranch(nn.Module):\n",
    "    def __init__(self, input_dim, out_channel, global_pool=\"GeM\", p=3, norm=None, dilation=True, hidden_channel=2048, image_size=(480, 640)):\n",
    "        super(LocalBranch, self).__init__()\n",
    "        # Dilation dependent on the parameter\n",
    "        self.dilation = dilation\n",
    "        # If no dilation, then start with first convolution layer\n",
    "        if self.dilation:\n",
    "            # 1024 -> 2048\n",
    "            # 512 + 512 + 512 + 512 = 2048\n",
    "            self.dilated_conv = DilatedConv(input_dim, hidden_channel, size=tuple(int(ti/16) for ti in image_size))\n",
    "            self.conv1x1_1 = nn.Conv2d(hidden_channel, out_channel, kernel_size=1)\n",
    "            # 2048 -> 2048\n",
    "        else:\n",
    "            # 1024 -> 2048\n",
    "            self.conv1x1_1 = nn.Conv2d(input_dim, out_channel, kernel_size=1)\n",
    "\n",
    "        self.conv1x1_2 = nn.Conv2d(\n",
    "            out_channel, out_channel, kernel_size=1, bias=False)\n",
    "        self.conv1x1_3 = nn.Conv2d(out_channel, out_channel, kernel_size=1)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.bn = nn.BatchNorm2d(out_channel)\n",
    "        self.softplus = nn.Softplus()\n",
    "\n",
    "        # We can apply any type of pooling to the local features.\n",
    "        # GeM is the default to be consistent with the global features.\n",
    "        if global_pool == \"max\":\n",
    "            self.pool = nn.AdaptiveMaxPool2d(output_size=(1, 1))\n",
    "        elif global_pool == \"avg\":\n",
    "            self.pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))\n",
    "        elif global_pool == \"GeM\":\n",
    "            self.pool = GeM(p=p)\n",
    "        else:\n",
    "            self.pool = None\n",
    "        self.norm = norm\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.dilation:\n",
    "            x = self.dilated_conv(x)\n",
    "\n",
    "        local_feat = self.conv1x1_1(x)\n",
    "        local_feat = self.relu(local_feat)\n",
    "        local_feat = self.conv1x1_2(local_feat)\n",
    "        local_feat = self.bn(local_feat)\n",
    "\n",
    "        attention_map = self.relu(local_feat)\n",
    "        attention_map = self.conv1x1_3(attention_map)\n",
    "        attention_map = self.softplus(attention_map)\n",
    "\n",
    "        # We don't apply normalisation by default, just like GCL\n",
    "        if self.norm == \"L2\":\n",
    "            local_feat = F.normalize(local_feat, p=2, dim=1)\n",
    "\n",
    "        local_feat = local_feat * attention_map\n",
    "\n",
    "        # Apply GeM pooling, to reduce the vector dimensions to [1, 2048]\n",
    "        local_feat = self.pool.forward(local_feat)\n",
    "\n",
    "        return local_feat"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_proj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15 | packaged by conda-forge | (main, Nov 22 2022, 08:48:25) \n[Clang 14.0.6 ]"
  },
  "vscode": {
   "interpreter": {
    "hash": "2eded2f99aeb90719e888b4c5be52669e449fec0a21fe7ddff5c2d5ec2cc4c84"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
