{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2c9cf8-7f2c-4913-87ba-918f5788e44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "from generalized_contrastive_loss.datasets import *\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b55db1-2946-4ada-9c4c-a8f4a8425e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeM(nn.Module):\n",
    "    def __init__(self, p=3, eps=1e-6, requires_grad=False):\n",
    "        super(GeM,self).__init__()\n",
    "        self.p = nn.Parameter(torch.ones(1)*p, requires_grad=requires_grad)\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.gem(x, p=self.p, eps=self.eps)\n",
    "\n",
    "    def gem(self, x, p=3, eps=1e-6):\n",
    "        return F.avg_pool2d(x.clamp(min=eps).pow(p), (x.size(-2), x.size(-1))).pow(1./p)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + '(' + 'p=' + '{:.4f}'.format(self.p.data.tolist()[0]) + ', ' + 'eps=' + str(self.eps) + ')'\n",
    "\n",
    "\n",
    "class BaseNet(nn.Module):\n",
    "    def __init__(self, backbone, global_pool=None, poolkernel=7, norm=None, p=3, num_clusters=64):\n",
    "        super(BaseNet, self).__init__()\n",
    "        self.backbone = backbone\n",
    "        for name, param in self.backbone.named_parameters():\n",
    "                n=param.size()[0]\n",
    "        self.num_features=n\n",
    "        self.pretrained_cfg = {}\n",
    "        self.num_classes=0\n",
    "        if global_pool == \"max\":\n",
    "            self.pool = nn.AdaptiveMaxPool2d(output_size=(1, 1))\n",
    "        elif global_pool == \"avg\":\n",
    "            self.pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))\n",
    "        elif global_pool == \"GeM\":\n",
    "            self.pool=GeM(p=p)\n",
    "        else:\n",
    "            self.pool = None\n",
    "        self.norm=norm\n",
    "\n",
    "    # This function returns both local and global features\n",
    "    def forward(self, x0):\n",
    "\n",
    "        # conv1\n",
    "        x0 = self.backbone[0](x0)\n",
    "        # bn1\n",
    "        x0 = self.backbone[1](x0)\n",
    "        # relu\n",
    "        x0 = self.backbone[2](x0)\n",
    "        # max0pool\n",
    "        x0 = self.backbone[3](x0)\n",
    "        # layer1\n",
    "        x0 = self.backbone[4](x0)\n",
    "        # layer2\n",
    "        x0 = self.backbone[5](x0)\n",
    "        # layer3\n",
    "        local_features = self.backbone[6](x0)\n",
    "        # print(f\"local feature size {local_features.size()}\")\n",
    "        \n",
    "        # layer4. This is equivalent to do out = self.backbone.forward(x0)    \n",
    "        # We don't extract the global features here because we have already done that\n",
    "        # global_features = self.backbone[7](local_features)\n",
    "        # Apply GeM pooling on the features from layer 4 \n",
    "        # global_features = self.pool.forward(global_features).squeeze(-1).squeeze(-1)\n",
    "        \n",
    "\n",
    "        # print(f\" local features shape before pooling\", local_features.shape)\n",
    "        # Apply GeM pooling on the local features \n",
    "        # local_features = self.pool.forward(local_features)\n",
    "        # print(f\" local features shape after pooling\", local_features.shape)\n",
    "        \n",
    "        # Doing this for debug purposes. the following commands could be combined\n",
    "        local_features =local_features.squeeze(-1)\n",
    "        # print(f\" local features shape after one time squeezing\", local_features.shape)\n",
    "\n",
    "        local_features =local_features.squeeze(-1)\n",
    "        # print(f\" local features shape after two times squeezing\", local_features.shape)\n",
    "        \n",
    "        \n",
    "\n",
    "        if self.norm == \"L2\":\n",
    "            global_features=nn.functional.normalize(global_features)\n",
    "        return local_features\n",
    "\n",
    "\n",
    "class SiameseNet(BaseNet):\n",
    "    def __init__(self, backbone, global_pool=None, poolkernel=7,norm=None, p=3,num_clusters=64):\n",
    "        super(SiameseNet, self).__init__(backbone, global_pool, poolkernel, norm=norm, p=p,num_clusters=num_clusters)\n",
    "\n",
    "    def forward(self, x0, x1):\n",
    "        out0 = super(SiameseNet, self).forward(x0)\n",
    "        out1 = super(SiameseNet, self).forward(x1)\n",
    "        return out0, out1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f396d8-af29-4a04-a795-71a77f601ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader(dataset, root_dir, idx_file, gt_file, image_t, batch_size):\n",
    "    # Create dataset\n",
    "    if dataset==\"test\":\n",
    "        ds = TestDataSet(root_dir, idx_file, transform=image_t)\n",
    "        return DataLoader(ds, batch_size=batch_size, num_workers=4)\n",
    "\n",
    "    if dataset == \"soft_siamese\":\n",
    "        ds = SiameseDataSet(root_dir, idx_file, gt_file, ds_key=\"fov\", transform=image_t)\n",
    "    elif dataset == \"binary_siamese\":\n",
    "        ds = SiameseDataSet(root_dir, idx_file, gt_file, ds_key=\"sim\", transform=image_t)\n",
    "    return DataLoader(ds, batch_size=batch_size, num_workers=4, shuffle=True)\n",
    "\n",
    "\n",
    "def get_backbone(name):\n",
    "    if name == \"resnet18\":\n",
    "        backbone = models.resnet18(pretrained=True)\n",
    "    elif name == \"resnet34\":\n",
    "        backbone = models.resnet34(pretrained=True)\n",
    "    elif name == \"resnet152\":\n",
    "        backbone = models.resnet152(pretrained=True)\n",
    "    elif name == \"resnet50\":\n",
    "        backbone = models.resnet50(pretrained=True)\n",
    "    if name == \"densenet161\":\n",
    "        backbone = models.densenet161(pretrained=True).features\n",
    "        output_dim=2208\n",
    "    elif name == \"densenet121\":\n",
    "        backbone = models.densenet121(pretrained=True).features\n",
    "        output_dim=2208\n",
    "    elif name == \"vgg16\":\n",
    "        backbone = models.vgg16(pretrained=True).features\n",
    "        output_dim=512\n",
    "    elif name == \"resnext\":\n",
    "        backbone = torch.hub.load('facebookresearch/WSL-Images', 'resnext101_32x8d_wsl')\n",
    "        # Supposed to be ['conv1', 'bn1', 'relu', 'maxpool', 'layer1', 'layer2', 'layer3', 'layer4', 'avgpool', 'fc']\n",
    "        print(f\" the layers of the resnext101_32x8d_wsl are: {backbone._modules.keys()}\")\n",
    "    if \"resne\" in name:\n",
    "        backbone = torch.nn.Sequential(*(list(backbone.children())[:-2]))\n",
    "        output_dim = 2048\n",
    "        print(f\" the layers of the resnext101_32x8d_wsl are after removing the last two layers (avgpool and fc): {backbone._modules.keys()}\")\n",
    "    return backbone, output_dim\n",
    "\n",
    "\n",
    "def create_model(name, pool, last_layer=None, norm=None, p_gem=3, num_clusters=64, mode=\"siamese\"):\n",
    "    backbone, output_dim = get_backbone(name)\n",
    "    layers = len(list(backbone.children()))\n",
    "    print(f\"Number of layers: {layers}\")\n",
    "\n",
    "    if last_layer is None:\n",
    "        last_layer = layers\n",
    "    elif \"densenet\" in name:\n",
    "        last_layer=last_layer*2\n",
    "    elif \"vgg\" in name:\n",
    "    \tlast_layer=last_layer*8-2\n",
    "    aux = 0\n",
    "    for c in backbone.children():\n",
    "\n",
    "        if aux < layers - last_layer:\n",
    "            print(aux, c._get_name(), \"IS FROZEN\")\n",
    "            for p in c.parameters():\n",
    "                p.requires_grad = False\n",
    "        else:\n",
    "            print(aux, c._get_name(), \"IS TRAINED\")\n",
    "        aux += 1\n",
    "    if mode==\"siamese\":\n",
    "        return SiameseNet(backbone, pool, norm=norm, p=p_gem, num_clusters=num_clusters)\n",
    "    elif mode==\"triplet\":\n",
    "        return TripletNet(backbone, pool, norm=norm, p=p_gem, num_clusters=num_clusters)\n",
    "    else:\n",
    "        return BaseNet(backbone, pool, norm=norm, p=p_gem)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b29506f-b235-4c6f-bfa4-e3d6140b2230",
   "metadata": {},
   "source": [
    "### Local branch model\n",
    "#### MultiAtrous + Attention maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed89060-2ae4-49a9-a68d-1619959b1c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiAtrous(nn.Module):\n",
    "    def __init__(self, in_channel, out_channel, size, dilation_rates=[6, 12, 18]):\n",
    "        super().__init__()\n",
    "        self.dilated_convs = [\n",
    "            nn.Conv2d(in_channel, int(out_channel/4),\n",
    "                      kernel_size=3, dilation=rate, padding=rate)\n",
    "            for rate in dilation_rates\n",
    "        ]\n",
    "        self.gap_branch = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Conv2d(in_channel, int(out_channel/4), kernel_size=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Upsample(size=size, mode='bilinear')\n",
    "        )\n",
    "        self.dilated_convs.append(self.gap_branch)\n",
    "        self.dilated_convs = nn.ModuleList(self.dilated_convs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        local_feat = []\n",
    "        for dilated_conv in self.dilated_convs:\n",
    "            local_feat.append(dilated_conv(x))\n",
    "        local_feat = torch.cat(local_feat, dim=1)\n",
    "        return local_feat\n",
    "\n",
    "\n",
    "class LocalBranch(nn.Module):\n",
    "    def __init__(self, input_dim, out_channel, global_pool=\"GeM\",p=3,norm=None, hidden_channel=2048, image_size=(480, 640)):\n",
    "        super().__init__()\n",
    "        self.multi_atrous = MultiAtrous(input_dim, hidden_channel, size=tuple(int(ti/16) for ti in image_size))\n",
    "        self.conv1x1_1 = nn.Conv2d(hidden_channel, out_channel, kernel_size=1)\n",
    "        self.conv1x1_2 = nn.Conv2d(\n",
    "            out_channel, out_channel, kernel_size=1, bias=False)\n",
    "        self.conv1x1_3 = nn.Conv2d(out_channel, out_channel, kernel_size=1)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.bn = nn.BatchNorm2d(out_channel)\n",
    "        self.softplus = nn.Softplus()\n",
    "        \n",
    "        # We can apply any type of pooling to the lf before returning them back to the use. GeM is the default to be consistent\n",
    "        # with the global features.\n",
    "        if global_pool == \"max\":\n",
    "            self.pool = nn.AdaptiveMaxPool2d(output_size=(1, 1))\n",
    "        elif global_pool == \"avg\":\n",
    "            self.pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))\n",
    "        elif global_pool == \"GeM\":\n",
    "            self.pool=GeM(p=p)\n",
    "        else:\n",
    "            self.pool = None\n",
    "        self.norm = norm    \n",
    "\n",
    "    def forward(self, x):\n",
    "        local_feat = self.multi_atrous(x)\n",
    "\n",
    "        local_feat = self.conv1x1_1(local_feat)\n",
    "        local_feat = self.relu(local_feat)\n",
    "        local_feat = self.conv1x1_2(local_feat)\n",
    "        local_feat = self.bn(local_feat)\n",
    "\n",
    "        attention_map = self.relu(local_feat)\n",
    "        attention_map = self.conv1x1_3(attention_map)\n",
    "        attention_map = self.softplus(attention_map)\n",
    "        \n",
    "        # We don't apply normalisation by default, just like GCL\n",
    "        if self.norm == \"L2\":\n",
    "            local_feat=F.normalize(local_feat)\n",
    "            # local_feat = F.normalize(local_feat, p=2, dim=1)\n",
    "        \n",
    "        local_feat = local_feat * attention_map\n",
    "        \n",
    "        # Apply GeM pooling, standard to reduce the vector dimensions to [1,2048]\n",
    "        local_feat =  self.pool.forward(local_feat).squeeze(-1).squeeze(-1)\n",
    "        \n",
    "        return local_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63cb0fe-a096-42c3-a3bb-5728d81d9192",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "cbf36edf16826f338fcddf9817feee204e82a9d331b7cc35297b5ee4d265a83d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
